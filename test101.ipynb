{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import concurrent.futures\n",
    "import openai\n",
    "import json\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load your dataset\n",
    "data = pd.read_json(\"C:\\\\Users\\\\chiso\\\\Downloads\\\\demo-data-setup-documents\\\\reduced_file.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>article_locations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gang of five convicted of human trafficking an...</td>\n",
       "      <td>[Glasgow, Romania, Scotland, Scotland, Scotland]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nebraska officials again sound alarm on human ...</td>\n",
       "      <td>[Nebraska, Nebraska, Nebraska, Omaha, Nebraska...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gang of five convicted of human trafficking an...</td>\n",
       "      <td>[Glasgow, Romania, Scotland, Scotland, Scotland]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gang of five convicted of human trafficking an...</td>\n",
       "      <td>[Glasgow, Romania, Scotland, Scotland, Scotland]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How Texans Can Help Fight Human Trafficking Ac...</td>\n",
       "      <td>[Dallas, Texas, Texas]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Gang of five convicted of human trafficking an...   \n",
       "1  Nebraska officials again sound alarm on human ...   \n",
       "2  Gang of five convicted of human trafficking an...   \n",
       "3  Gang of five convicted of human trafficking an...   \n",
       "4  How Texans Can Help Fight Human Trafficking Ac...   \n",
       "\n",
       "                                   article_locations  \n",
       "0   [Glasgow, Romania, Scotland, Scotland, Scotland]  \n",
       "1  [Nebraska, Nebraska, Nebraska, Omaha, Nebraska...  \n",
       "2   [Glasgow, Romania, Scotland, Scotland, Scotland]  \n",
       "3   [Glasgow, Romania, Scotland, Scotland, Scotland]  \n",
       "4                             [Dallas, Texas, Texas]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load the spaCy NLP model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Function to extract locations using NER\n",
    "def extract_locations(text):\n",
    "    doc = nlp(text)  # Process text through spaCy's NLP pipeline\n",
    "    locations = []\n",
    "    for ent in doc.ents:\n",
    "        # Only extract GPE (Geopolitical Entities), and exclude persons (PERSON label)\n",
    "        if ent.label_ == \"GPE\" and ent.label_ != \"PERSON\":\n",
    "            locations.append(ent.text)\n",
    "    return locations\n",
    "\n",
    "# Function to process locations in parallel\n",
    "def extract_locations_parallel(df, column_name):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Apply the extract_locations function in parallel for the specified column\n",
    "        result = list(executor.map(extract_locations, df[column_name]))\n",
    "    return result\n",
    "\n",
    "# Apply location extraction to the article column\n",
    "data['article_locations'] = data['article'].apply(extract_locations)\n",
    "\n",
    "# Check extracted locations\n",
    "data[['title', 'article_locations']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize GeoPy geocoder\n",
    "geolocator = Nominatim(user_agent=\"myApp\", timeout=10)\n",
    "\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "\n",
    "# Mapping countries to English\n",
    "country_mapping = {\n",
    "    'پاکستان': 'Pakistan',\n",
    "    'دولة الكويت': 'Kuwait',\n",
    "    'روما': 'Romania',\n",
    "    'ليبيا': 'Libya',\n",
    "    'اليونان': 'Greece',\n",
    "    # Add more translations here as necessary\n",
    "}\n",
    "\n",
    "def get_country_from_city(city):\n",
    "    try:\n",
    "        location = geolocator.geocode(city)\n",
    "        if location:\n",
    "            country = location.address.split(\",\")[-1].strip()\n",
    "            # Translate country to English if it's in a non-English script\n",
    "            return country_mapping.get(country, country)  # Default to country if no mapping exists\n",
    "        else:\n",
    "            return None\n",
    "    except GeocoderTimedOut:\n",
    "        print(f\"Timeout error occurred while geocoding {city}. Retrying...\")\n",
    "        return get_country_from_city(city)  # Retry the request\n",
    "    except Exception as e:\n",
    "        print(f\"Error resolving location {city}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a location is valid (city/country) and return with country\n",
    "def get_location_with_country(location):\n",
    "    try:\n",
    "        country = get_country_from_city(location)\n",
    "        if country:\n",
    "            return {\"city\": location, \"country\": country}\n",
    "        else:\n",
    "            return None\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter out non-geographical locations (cities and countries only)\n",
    "def filter_geographical_locations(locations):\n",
    "    if locations is None:\n",
    "        return {\"start\": [], \"intermediate\": [], \"end\": []}\n",
    "\n",
    "    filtered_locations = {\"start\": [], \"intermediate\": [], \"end\": []}\n",
    "\n",
    "    for category in ['start', 'intermediate', 'end']:\n",
    "        for location in locations.get(category, []):\n",
    "            location_with_country = get_location_with_country(location)\n",
    "            if location_with_country:\n",
    "                filtered_locations[category].append(location_with_country)\n",
    "\n",
    "    return filtered_locations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI API\n",
    "openai.api_key = \"Add urs\"\n",
    "\n",
    "# Refined function to extract traffic flow using ChatGPT with better contextual understanding\n",
    "def extract_traffic_flow_chatgpt_with_context(article):\n",
    "    prompt = f\"\"\"\n",
    "    Given the following article, classify the locations into start, intermediate, and end locations. Distinguish between locations where victims were actually trafficked and locations that were only promised. Exclude non-geographical locations, such as organizations, departments, or unspecified places.\n",
    "\n",
    "    Article: \"{article}\"\n",
    "\n",
    "    Please answer the following:\n",
    "    1. Classify actual trafficking flow into 'start', 'intermediate', and 'end' locations.\n",
    "    2. If a location is mentioned as a promised destination but victims were not actually trafficked to it, exclude it from 'intermediate' and 'end' locations.\n",
    "    3. Return the output in JSON format with 'start', 'intermediate', and 'end' locations, like this:\n",
    "    {{\n",
    "        \"start\": [\"City1\"],\n",
    "        \"intermediate\": [\"City2\", \"City3\"],\n",
    "        \"end\": [\"City4\"]\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    # Send the prompt to OpenAI API (chat model)\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",  # Use the latest model\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that understands human trafficking flows and can distinguish between actual trafficking routes and promised destinations.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=150,\n",
    "        temperature=0.5,\n",
    "    )\n",
    "\n",
    "    # Print the raw response for debugging\n",
    "    raw_response = response['choices'][0]['message']['content'].strip()\n",
    "    print(f\"Raw response: {raw_response}\")\n",
    "\n",
    "    # Parse the response and handle any errors\n",
    "    try:\n",
    "        traffic_flow = json.loads(raw_response)\n",
    "        return traffic_flow\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing response: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw response: {\n",
      "    \"start\": [\"Paris\"],\n",
      "    \"intermediate\": [\"Berlin\"],\n",
      "    \"end\": [\"Madrid\"]\n",
      "}\n",
      "Raw response: {\n",
      "    \"start\": [\"Johannesburg\"],\n",
      "    \"end\": [\"Cape Town\"]\n",
      "}\n",
      "Raw response: {\n",
      "    \"start\": [\"Gujranwala\"],\n",
      "    \"intermediate\": [],\n",
      "    \"end\": [\"Gujrat\"]\n",
      "}\n",
      "                title  \\\n",
      "0  Trafficking Case 1   \n",
      "1  Trafficking Case 2   \n",
      "2  Trafficking Case 3   \n",
      "\n",
      "                                                          traffic_flow  \\\n",
      "0  {'start': ['Paris'], 'intermediate': ['Berlin'], 'end': ['Madrid']}   \n",
      "1                    {'start': ['Johannesburg'], 'end': ['Cape Town']}   \n",
      "2     {'start': ['Gujranwala'], 'intermediate': [], 'end': ['Gujrat']}   \n",
      "\n",
      "                                                                                                                                                   filtered_traffic_flow  \n",
      "0  {'start': [{'city': 'Paris', 'country': 'France'}], 'intermediate': [{'city': 'Berlin', 'country': 'Deutschland'}], 'end': [{'city': 'Madrid', 'country': 'España'}]}  \n",
      "1                        {'start': [{'city': 'Johannesburg', 'country': 'South Africa'}], 'intermediate': [], 'end': [{'city': 'Cape Town', 'country': 'South Africa'}]}  \n",
      "2                                     {'start': [{'city': 'Gujranwala', 'country': 'Pakistan'}], 'intermediate': [], 'end': [{'city': 'Gujrat', 'country': 'Pakistan'}]}  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample data (replace with actual dataset)\n",
    "sample_data = pd.DataFrame({\n",
    "    \"title\": [\"Trafficking Case 1\", \"Trafficking Case 2\", \"Trafficking Case 3\"],\n",
    "    \"article\": [\n",
    "        \"The victim was moved from Paris to Berlin before arriving in Madrid.\",\n",
    "        \"Authorities rescued victims who traveled from Johannesburg to Cape Town.\",\n",
    "        \"The Federal Investigation Agency (FIA) Gujranwala Zone on Friday apprehended five individuals involved in human trafficking and visa fraud during raids in Gujranwala and Gujrat.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Apply the ChatGPT-based traffic flow prediction with context\n",
    "sample_data['traffic_flow'] = sample_data['article'].apply(extract_traffic_flow_chatgpt_with_context)\n",
    "\n",
    "# Apply the filtering function to the extracted traffic flow\n",
    "sample_data['filtered_traffic_flow'] = sample_data['traffic_flow'].apply(filter_geographical_locations)\n",
    "\n",
    "# Display the results\n",
    "pd.set_option('display.max_colwidth', None)  # Prevent truncation\n",
    "print(sample_data[['title', 'traffic_flow', 'filtered_traffic_flow']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the ChatGPT-based traffic flow prediction\n",
    "data['traffic_flow'] = data['article'].apply(extract_traffic_flow_chatgpt)\n",
    "\n",
    "# Apply the filtering function to the extracted traffic flow\n",
    "data['filtered_traffic_flow'] = data['traffic_flow'].apply(filter_geographical_locations)\n",
    "\n",
    "# Display the results\n",
    "pd.set_option('display.max_colwidth', None)  # Prevent truncation\n",
    "print(data[['title', 'filtered_traffic_flow']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# OpenAI API Key\n",
    "openai.api_key = \"your-openai-api-key\"\n",
    "\n",
    "# Initialize GeoPy geocoder\n",
    "geolocator = Nominatim(user_agent=\"myApp\", timeout=10)\n",
    "\n",
    "# Function to get the country from a city using GeoPy\n",
    "def get_country_from_city(city):\n",
    "    try:\n",
    "        location = geolocator.geocode(city)\n",
    "        if location:\n",
    "            country = location.address.split(\",\")[-1].strip()\n",
    "            return country\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error resolving location {city}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to get traffic flow using ChatGPT\n",
    "def extract_traffic_flow_chatgpt_with_country(article):\n",
    "    prompt = f\"\"\"\n",
    "    Given the following article, classify the locations into start, intermediate, and end locations.\n",
    "    Only include valid geographical locations such as cities and countries. Exclude any non-geographical terms or specific addresses.\n",
    "    Ensure the output includes both the city names and their corresponding countries.\n",
    "\n",
    "    Article: \"{article}\"\n",
    "\n",
    "    Example output format:\n",
    "    {{\n",
    "        \"start\": [{\"city\": \"City1\", \"country\": \"Country1\"}],\n",
    "        \"intermediate\": [{\"city\": \"City2\", \"country\": \"Country2\"}],\n",
    "        \"end\": [{\"city\": \"City3\", \"country\": \"Country3\"}]\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    # Send the prompt to OpenAI API (chat model)\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=150,\n",
    "        temperature=0.5,\n",
    "    )\n",
    "\n",
    "    # Get and clean the raw response\n",
    "    raw_response = response['choices'][0]['message']['content'].strip()\n",
    "    try:\n",
    "        traffic_flow = json.loads(raw_response)\n",
    "        return traffic_flow\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error parsing response: {raw_response}\")\n",
    "        return None\n",
    "\n",
    "# Sample dataset\n",
    "sample_data = pd.DataFrame({\n",
    "    \"title\": [\"Trafficking Case 1\", \"Trafficking Case 2\", \"Trafficking Case 3\"],\n",
    "    \"article\": [\n",
    "        \"The victim was moved from Paris to Berlin before arriving in Madrid.\",\n",
    "        \"Authorities rescued victims who traveled from Johannesburg to Cape Town.\",\n",
    "        \"The Federal Investigation Agency (FIA) Gujranwala Zone on Friday apprehended five individuals involved in human trafficking and visa fraud during raids in Gujranwala and Gujrat.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Apply the ChatGPT-based traffic flow prediction with context\n",
    "sample_data['traffic_flow'] = sample_data['article'].apply(extract_traffic_flow_chatgpt_with_country)\n",
    "\n",
    "# Apply the filtering function to the extracted traffic flow\n",
    "sample_data['filtered_traffic_flow'] = sample_data['traffic_flow'].apply(filter_geographical_locations_with_country)\n",
    "\n",
    "# Display the results\n",
    "print(sample_data[['title', 'traffic_flow', 'filtered_traffic_flow']].head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
